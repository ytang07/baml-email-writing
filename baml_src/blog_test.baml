test TestName {
  functions [ExtractQuestions]
  args {
    transcript #"
        Hey everyone, nice to join. I'm more waiting. I guess I joined a little early with a few other folks. I'm not sure. All right. Do you know if the Zoom chat persists for people that join later? I'm not sure. I don't see anything in the chat if you posted something. Oh, it does not. Okay. So I'll just keep posting it a little bit. We'll probably start around like 11 or 5-ish. And then we'll go on from there. Yeah, what's up? Hey, you, Jen. Wow. Last week on here. This is awesome. Yeah. We'll be fine. Good amount. And then while we're waiting, everyone, I'm going to post a really quick thing just so we know how to break out the questions and everything and how we address everything. So if you could, I'm going to start a quick little poll. Just to give myself an idea of what kind of tech we should use today for the demos. And what kind of question will be answering? If you could answer the four questions. That would be amazing to help us out here. Hey, how's it going? Hey. Well, it's good. We're we're hoping everyone that joined us mostly engineers. And it looks like most people actually in college. It's great. It's going to be much more fun. Okay, we got mostly Python. There's no chat. Well, is it chat GPT or is it cursor at this point, to be honest? What's team Cody? What's the quote on my wall on my wall? Not really my wall, I'm actually in an urban beat today. I'll show you the quotes on my walls later. But this one says when you realize nothing is lacking the whole world belongs to you. Well, see how that how true that is today when our prompts don't work. Live demos. I'm kind of scared of doing some other sales. And looks like most people have answered the survey. Well, I'll share the results for that. You seconds, too. I think we've we've like two more minutes. I'm going to start coding. Hey, pernae, nice for joining. And then I'm going to post this link over here. If you guys check it out, this is I put a post to that I send this over on the loom as well. But this is going to be like the general things of what we're going to talk about today. If you guys have things that you want to do in the demo, just add your own idea down below in the pre meeting section. And like we'll just pick one of the ideas that's out there. And then we'll put a community polling because then you know we're actually going to do it live and it'll be fun. A lot of people want to see the same thing. And then for any topics you have, feel free to change add new topics or questions as well. And just set the topic appropriately if you can put your name that'll help. So we can call out the person. If they if we have any follow up questions about them. I'm going to start. All right, I think we're ready. Let's do it. Well, with the new real time API. Well, chat about that in a second. You can discuss a bunch of questions down there. Cool. So first of all, I'm going to do a quick introduction. My name is Vibeau. I'm one of the co founders of boundary and we build a programming language that makes LMS a lot easier to use and a lot more consistent without any fine tuning. On my left, you'll see Aaron or I guess I don't know what order keep on, but you'll see Aaron Aaron can you wave and introduce yourself. Yes, I'm Aaron the other co founder. And yeah, happy to be here and show you how to improve your AI pipelines. Yeah. And then really the goal and I'll introduce for us on a second, but really the goal of these sessions is what I found is it's really fun to see other people do prompt engineering because sometimes you just find a different way of thinking about. And we learn a lot from seeing others think about the problem I've learned a lot from design when you're approaching. Doing his agent that works much or a lot. And just watching Aaron and me do it differently. It's also just expanded the way that I school problems. So the goal of the series is really just. Do some live coding. So awesome real problems. And then perhaps. We all just walk away with better Ellen pipelines coming out of it. And then now I want to do a quick introduction of a son who is building a really cool agentic framework that he built and in line of today's topic. The whole real goal here is to talk about how we build a genetic framework that scale and I think before we talk about it, we should talk about one that is built somewhat well. And look at what design decisions decisions were made in the process and how we can emulate them and the demo that we build today. Is that you want to do a quick introduction? Yeah, happy to do that and sort of give give screen share as well after I want my name is phase on I'm the co founder of show like AI. Show like is an AI co pilot for traders and investors. Our goal is to really democratize access to market experts to the power of AI and you know what a time to live in with large language models that we can make that possible. And yeah, I mean, you know, we've been building the agentic architecture for quite some time and we've tried everything under the roof with the land graph and so on. And ran into a very challenging problem with mixed structured outputs and that's what we decided to talk to. Yeah, yeah, happy you want to just kind of get into the. Yeah, let's just watch the agent. I think that's usually the best part if we just like watch what he can do in different scenarios. I was kind of impressed by a lot of the things that you came up with. Yeah, yeah, so I want more you guys with all the details of, you know, how our part works. You guys can check that out on your own time and I'll dive right into to sort of our agent. So yeah, again, like I said, I can really help you, you know, discover, analyze and trade anything across the stock market. So we have this feature that we call quick prompts. We notice a lot of people or the average person can be bad at prompt engineering. So we don't want them to prompt stuff within our product and just go through this quick prompt experience. So here you can just, you know, go through this flow, say, hey, I'm looking to trade options and looking for some trade setups. And then you'll see as soon as I submit, you'll see these like loading statements coming in that's all coming through the chat model, by the way. So in real time, you know, our battle agent is like kind of figuring out, hey, what data needs to be fetched and it figured that out. Give that message to the user and then once it had the data that I needed it performed the analysis and basically gave us a bunch of different response. Right. So it finally responded to that user after that. But there are all kinds of edge cases that we run into as well. Right. And the user asks a question, but doesn't really specify the stock, right. So they say, hey, some rides is the latest earnings. Oh, yeah. Can you zoom in, is on. It's like command chef plus plus. A little bit more. A little bit more. Yeah, there you go. I'll go to more actually. Right. I'll maybe run through that again, but hopefully this example resonate with a lot of you where you need to collect some user input and they've asked a question that's not complete. So you need to clarify questions, right. And that requires the model to take a different action, right. It's not the same action is just responding to the to the user with the analysis. And that's what the battle is really enabled for us to be able to take a different action. Right. And you just give it some more like specification. What it means. Yeah. So we're all just, you know, say Tesla. And you notice like here, I don't know if you guys saw, but it actually did some dot dots and started like actually going through some multiple steps to go show you. And that like UI difference of telling the user something is happening can really make a big difference in the accuracy of your agent. So the communication of the user on the accuracy of your agent as well. So in this case, it was able to determine that it had a follow up question to ask and then ask asked follow questions. But usually from what I have seen, the way that people usually do this is you write a massive prompt somewhere in your prompt. You're like, hey, if the user needs a follow up question, ask them for a follow up question. And that's it. You're smart. If it has all the information, go do go answer the question. But that can be a little impossible, right? Because if you want two different types of data and you can't, if you're looking for text versus something else, right, you can't even do that in some cases where the mixed output is very challenging to do. It's just one long prompt. And I think like the funniest thing I saw was like ask it to like generate a resume for you. Oh, yeah, I actually like have that exact. I was going to be one of my questions because I saw you test that the other day. I was like, oh, that's a cool test. So yeah, it's a question that makes no sense, right, for our platform, right? So we have some interesting like sort of fall back here. Just say, hey, I'm here. It's history with financial analysis. It kind of handles that. There's so many ways you can handle this, but prompting is just so hard. Did you try to bunch of prompting techniques before to make this work? Oh, yeah, I did. And the prompting techniques, it gets really complicated because, you know, I think maybe this is a good segue to kind of showing this diagram that puts together. You know, when a user asks the question, the chat model essentially needs to decide which action to take, right? That action also includes, by the way, data gathering, right, and calling some tools. So having that ability to decide what action to take. Do I need to gather the data and call some tools versus do I need to respond to the user or ask clarifying questions? It's just really hard to model. If you don't have something like BAMO to specify as as BAMO types and just map these actions to those BAMO types. So you keep talking about BAMO, but like I think the most interesting thing is actually just the fact of like when you as when I think about this problem, I think if we all look at this diagram statement, I'll annotate a little bit. This really looks like a really simple thing. This is if we were to think about in the terms of Python, because most people write Python code, you can view this as just a user input literally we can use it as like the input command mostly a fast I can type code. You can use this as the input command and as you're going about the input command, it really is like almost like an if statement over here. It's like if or like a switch statement, I guess the better one. Based on the input, I kind of want to do a switch to one of these three actions based on whatever the input is. So if the user said something like give me a bit of resume, you really just responding to the user with something with a little bit more specific and the response user prompt probably has something along the lines of if it's not about financial data, you just tell them you don't handle financial data. Exactly. On the other hand, if you're asking about Tesla, then the switch goes to like something about call tools where you actually go to a database, whole information about Tesla from there somehow, and then you will eventually go back into the switch statement and now if I have the information about Tesla, maybe I should respond to the user and go down this response and produce a good response. Now different and you can do this now we're in the switch statement with a little bit of a while loop if that makes sense to folks. Yeah. Now the other option is we go back to the very first question, Fesano showed is we asked tell summarize the latest earnings statements that one that we can say switch we have an action to take, but we don't have enough data. So we should probably ask a clarifying question and after asking a clarifying question, we don't actually opt out, we actually go, we exit the while and go back to the top level point, which is asking a real asking input. So this is almost like two wild loops around each other where the first one is a big while loop and the other one is another input does that kind of makes sense to folks by the way. And that's funny. But does this approach of how we kind of broke this problem down somewhat makes sense. Yeah, feel free to ask questions on the chat or like raise your hand or whatever you guys want. If you guys have questions up awesome. So this is the way that we have generally thought about this approach and set up yeah we'll do a code we're going to we're going to start coding really quickly right after this. But the general approach of whenever we say. Yeah, that's another good question question, I think. Okay, I'll go over a little bit the top of the while loop is about getting user input. So you're just asking the user for user input because at some point the user will give you input and at some point at the decide to exit the while from the user input. And the case of. Fizzons code you actually exit the while of all the time because you show from once you show a prompt to a user. You kind of exit and wait until the user gives you another message in this case it's driven by a button click or an enter in the UI. But you could just do it more than that right like the way I kind of think about it is actually is you exit the while loop anytime. So you respond to a user right safe. categorize respond to use and ask clarifying questions as user actions right which is like hey we need to do something with the user either respond to the user as a clarifying question. Any user action is going to lead us to exit the while loop right. If it isn't a user action that all we do is we then go back to our chat model with the data that we have collected from the tool calls right so the agent can define this decide what the next action is there right. So that's how we kind of like my mental model has been like is that a user action or is it a tool action right if it's a user action. Then just exit the while loop do something with the user if the tool action then call the tools and then get that response back to the agent. I think this is the this is sort of I think it's going to be a lot easier if we start writing some code actually. Yeah, mostly the demo of someone useful and letting people kind of see like one real application of building some agent architecture that's somewhat kind of robust. And in terms of like it can handle scenarios that are completely non related to the data it can handle scenarios where it doesn't have enough information and then they can also handle scenarios where it has the information now needs to give a decent response back to the models and we can talk about more interesting decent responses I think later but like generally wise and stuff they'll be kind of fun. And that's what we're looking at. We'll definitely showcase that at some point in the future as well. But one thing I just want to upload will this chart I know it's very specific to Sherlock. Yeah, you can model any agent in this way right you have a chat model the chat model has a set of actions that it can take right and then it takes those actions and those actions could be anything specific to your use case to your domain right in our use case is this financial domain and we're gathering data but it doesn't matter right. You have a chat model that takes some actions right and that's really the most sort of simple way to kind of think about this. Awesome so here's here's what we'll do really quickly well I think would be awesome is we put some things in there that are about some of the demos that we have so all screen share my screens for those of you that haven't seen this. I was going to share this specific. No, not all my windows. Just waiting for us to leak our IP I keys. Yeah, so we're going to we're going to do a really good job of not leaking IP I keys too much today that's the goal. Okay, so these are the four demo ideas we came up with today and I think what we can do for let's say your agent works through some email. And how can a while over that I'm going to answer that question on a second because I think one of the demos is going to help might help. So what I want to do today is I just want to write some code I think we we can talk about this conceptually but I think when we all see some real Python code it looks like Python is the winning language sorry that your folks. Python one. So we're going to write some Python code today we're going to make this. We're just going to see what it ends up working out so I'm going to end this poll. And I'm going to number these ideas. I need to come up here now. This works. I'm going to make it really quick. If anyone else wants to add some new ideas please now is the time we're literally coat up whatever you want. Fly as long as we can get some decent ish data for it. I'm going to launch another poll really fast and everything pick their numbers down. Does anyone else have any ideas that they want to add in there. Otherwise I'm going to launch this poll and then we'll pick one and we'll just start coding. I will take that as a. Oh, I'm not sharing this with the poll what I'm not sharing. Sorry. We don't answer your question right now. We'll send you an email or a message with. You can ask a question. Aaron can you just copy and paste questions onto the dock while they get posted that way we can just track them. And we'll make sure we go over them. Okay, I'm going to launch a poll. Let me know which poll you guys want to which demo you guys want to see. Identify what makes a profile good photo picture like perhaps like LinkedIn Tinder, pin whatever you guys want. That's for something on news article. Create a good response for a message on discord or. Most button numbers, fingers held up by hand. Oh, there's like a pretty pretty. We'll run this for about 30 more seconds and you can change your vote to do whatever you want. And we'll do this real time so it may not work with the real point of this demo is to really talk about like different thought process and different strategies to make this work. All right, last 10 seconds. Get your votes in if you guys want. 15 15 for that versus opinions of any of you on one and two want to shift it the other way. This is your last chance. Well, I'm going to do it. All right, that's versus opinions of this. That's what we're doing. If you guys are curious, I guess I can share the points. That's kind of what the poll results were. It looks like people are very close between one and four three and four. So maybe we'll do four next time. Let's do the news article thing. So get started. I'm going to just screen share my screen. I guess. Well, I'm going to get the chat up and running. And then we will start. We're going to start with this. So what I have over here that I start out is I have a basic Python project. I have a basic Python project is Python project is set up to just have almost no code. I just made sure that I can actually run stuff for those the yet that don't know about UV UV is just a great package manager for Python. Definitely look into it. We have a small read me of how to set it up. It's really in rust like BAML. I have a soft space for everything in rust. And what we're going to do now is the first thing we're going to do it. And I'd love for you guys to kind of be engaged in this and kind of give ideas around this problem of like back to versus opinions and news articles. So the first thing we should do is let's just start pulling up some news articles that we can use as test cases because we should know we're going after. Let's go. Do you have the repo available? Yeah, I'll link the repo in a second actually that's a good call. What can you just call them and they can you close the chat. It's boundary of mouse slash slash boundary mouse slash prompt shepherds. Okay. Yeah. Yeah. Very good. Great. So anyway, let's find some articles and then let's just go figure out what's going on. Like Elmo, what the heck is this? And what I'm going to do is I'm just going to start copying stuff. Because why not. I think this is the way to go about this. This looks like actually a dumb news article. Let's find an actual news article. Let's take this one. Okay. So we're going to find this. And this looks like a strong opinion piece. And then for sir. So I'm going to make a new folder called tests. You can see it in one dot 60. And before you even really go about this problem, the first thing that I always do whenever I go about any prompting and prompt is just come over test cases and then really just think through where I think the thing could be like really mess up. Really lobsters and mess. Let's talk about it lobster. Do you guys upfront have a couple of ideas how you would think about how you would think about these news articles and like what would make something a fact versus an opinion, like just put some in the chat really fast. And I think that's going to be helpful while I go and collect some data on this. And then I'll just go back to the first one. Curse or file CNN to take the. And then let's find another one who wants to write let's go to read it. Why not. Read it probably some opinion pieces. Okay, this looks like. This looks like. I think. Give me something better. Try an image of an article. Oh, we can do an image. Yeah, why don't I take an image. Okay, this is. That's good. BBC or something. Yeah. Yeah. Yeah, there we go. That's a good one. That's a good one. That's a good one. I just have like this screenshot or something. You can see. It's like an image that's a good idea. BBC, one, like, Angie, cool. So I have a couple test cases here. I have an article that doesn't look like a news article at all. Oh, let's take one that actually isn't a news article. Really fast at all. So like, which is to say, like, write it. So I take it in this. Awesome. And then I'm going to chat really fast. Cool. So let's take a look at what makes something of fat versus fat versus been in. So I think there's a couple of really, really good ideas in here already that are going to help inspire this thought process. So the first thing I'm going to say is that things like I think, yeah, the concept, I think is good. The call of it's in. Oh, yeah, like where did the data source from the person quoting it? Like quotes are generally factual, but the content of the quote might be non-vacual, which is kind of interesting. Could we handle. Could we handle the multimodal data like Twitter, Twitter post screen shot? Oh, that's where the image came from. Cool. So yeah, we'll do multimodal really fast. And then we have verify about cross multiple articles. And that's a really interesting. I think that's a really cool advanced concept of like, hey, if we see the same thing happening in like five different posts, it's probably true. So let's start with this and just see what we would go about this. The first thing that we think about whenever we start writing this is, we're going to go at BAML. So we'll do UV add, is it UV add earn? Yeah. BAML pie. And what does add the BAML pie project. And then secondarily, we're going to go ahead and just create a quick BAML starter by doing UV run, BAML CLI, dash in it. And what this does is it just adds BAML to my project. And what BAML really does for you is it just makes prompting a little bit easier. And more importantly, just allows me to think my my my prompts in a structured way. What does it mean to be structured? The idea of structured prompting is instead of asking the LLM to just respond in like a chat format, like you do with chat GBT. So I'll type that in. Can you guys hear I'll zoom in? I'll start typing the comments in here. Oh, thanks Aaron. That'll be easier for me because I can't run that zoom level. Cool. So the first things that I think about whenever I approach this is, I don't really want the LLM to respond freely. I don't want it to go to that. What I really want to say is, I want the LLM to make a decision for me of, where is the article coming from? Is it a new source? Is it a random text coming in from it? Is it an opinion piece? And I want to figure that out. And now that I've added BAML to my project, all I want to do is start playing around with some of this data and see what kind of prompt I should have. I think one of the first few things that we probably should think about is, where is it coming from that? The concept of where is the column in? Because if I know that it's like coming from Reddit versus CNN versus BBC versus some other source. I already have some content. We have a couple of ways that we can do this, but I think the magic of thinking about LLM is, what if everything doesn't have to come from an LLM? So for example, if I was writing a, if I was writing a scraper that did this, and I already knew where it comes from. I already know that it comes from CNN. I already know that it comes from BBC. So we could just skip that step. But in some cases, what does the user just pasted all this text into my into some box? And now I finally got other responses. In that case, I don't really know where it's coming from. It's arbitrary. So let's write a function that can go solve that problem for me. Let's write a function really quickly, new file. Find source.bangle. And some function that can be termed in where the source of this. Fontan, when you use me, you're like, okay, now before we do this and actually start filling out this function, let's start writing some Python code just to grasp our heads around what we might end up building. Let's assume we have a magical function that can do this death find source. And if we give it some text content, which is a stereotype, it will return to me something that is on the idea of like some sort of source concept. I'll return a source to me and I don't know what a source comes from, but let's just say source is an enum. That is a source. I do. Well, I forgot Python. You know, and what we're going to write here is going to write some like this CNN, BBC, I could ask her BBC Reddit. Twitter and maybe a couple more news media and what it's called. We'll call the rest of it. And we'll see what it comes from. Maybe we have like news coming from Facebook. We'll take a Facebook, we'll shut off my Facebook, I guess, and make that work. And we can go make these work. Why is this complaining? Let's ask her sort of fixes. I'm not a Python developer. I read mostly non-pipon code. And now we have a source. So if we had a system that could tell us what the source was, we might be able to do interesting things. Let's try this out like that. Source equals fine source. That const. Content equals. All this post out over here. And we'll forget that. Now, if we had the source, let's go back to some of the other ideas that we have. We had some ideas about how many quotations it has, how many facts it has. And ideally, I think let's just come up with like what we would like to do at the very end. What we'd love to do is. For back and facts. Print. Right. Yeah. For. Opinions are. And. Right. Yeah, the point is to see this as like a program, right? Like you just want to find little pieces of data and figure out how you're going to show that in your program. Yeah. So now that we've got this, let's figure out how we can get some facts and stuff out of this. And one way to do this is to write a symbol prompt that's going to write something like this where we have something that can like. Extract facts from content. And opinions from content. Let's go write this. That's why these functions really fast. And usually you do like a first pass, right? It's like, hey, let's just see what. What problem we can get that like we'll just work out of the box and then you start breaking down the problem. The story is really good about being on the paper you have. Cool. So I think your whole concept of relation between facts, people and documents is a really, really good concept but like. Who's involved in this content? So maybe we want to know a fact isn't simply just a fact. We're going to write a fact as something that has a little bit more involved in it. Let's just try and iterate on this tiny piece of content. I think that's going to help us get to a starting point. Because while we had this code, it may not work as well. And we should figure out for sure if it does or not. And I'm going to put some to buy this here, like, bash just to make my life easier. And actually, we'll also see it. OK, cool. OK, so let's see how we can implement, and then we'll also say, does this work? Article is from, but let's see how we can go make this work really fast. Now, we could write a prompt. But I think let's write a prompt in a slightly easier way. And we find that way to be damal. So when we write damal code, all we're going to do is we're going to write a function. A function can be called find source, or I guess in this case, identify source. It's going to take in some content, which is a string type. And then it's going to output a source type. Let's talk about what a source type is. We already have what a source type is. We're going to do it from right here. And I'll put this over to the side. And we're going to write an in-on source. Cool. Let's see what else. And we'll return the source. And in this case, I'm going to use a very simple model. I'll open it GPD, or a mini. And the reason I'm open to using such a simple model here is because this task itself is very simple. I don't think it will be that hard for a model to pick out what this is. So we shouldn't overcomplicate the complexity of the task. And we need a bigger model, we'll try it. And we'll just answer this. And all we have to do over here is we just write CTX.out for format. And we'll say identify. Don't define where this. Article is from. Cool. And then we'll add the rest in the user message. And just put in the content. Perfect. Now, the first thing that we probably want to do is, before even running our Python tests, we really want to be able to see how we go do this. And just run this test case really fast, because I already have some content over here. Let's see if I can try this out. We have a quick image over here. We have some CNN test cases. So let's make a test case test. Test space. Functions. Actually, I'll do it in a user way. This thing will actually give me the math for a test case. And I can just paste this over here. And I can just bring in the content from CNN and just try this out. And let's just see what this model does. Now, before we talk about more of what's going on here, I'm just going to give you a brief or ergonomic field of what's actually happening. Well, we're actually able to see is we can actually see the entire coping and entire prompt. That's put it into here. So CTX output format is actually giving me the information about that return type that I'm returning. And that's actually making this happen. So if I go change this, like I skip CNN notice that CNN isn't long here. It's a lot of real time previewed actual prompt. We're actually injecting any variables and stuff that you might put on here. And we're rendering it for you as well. And if you wanted to go see the actual web request that we're making under the hood. It's going to rule my API keys. Let me go add an API key really fast. Sorry. Well, I will do my best not to rule my API key. You're close. Aaron, can you take that question? Yeah, so CTX is just like a helper utility to print out different utilities. Like one of them is output format. We may have others in the future. So yeah, in this case is printing out the type source from this function definition. So if you right now we have an enum. So it'll print out like the list of enums. But if you have a class, it'll print out like the actual schema. Yeah, exactly. People have their screenshot tool ready. Yeah, and we'll see if I do it. Yeah. Where did the system from come from in the playground? So the default prompt here that we inject is the system prompt. And then to change that, you can just add this role user to change it to a user role. Well, actually, based on the model that you're using, you can choose the default as well. Like, for example, if you use images, you cannot use a system prompt with an image. It needs to be a user. Yeah, so system is like the default. So imagine there's just like a curly curly role system at the top here. We just inject that for you because we need a default. Yeah, so here we have a system and an user. That's kind of what we do for you. We also did that stuff. So like everything is just nice and tidy. Like no extra weird white spaces like in Python. Everything is just more readable. And you know, you got a question. So. Yeah, you can use you can use any local model. What we're just using the base models and they'll do an unmute and ask your question. Or I did. Oh, sorry, I've done your hand. I didn't mean to do that. Okay. Anyway, you have another question. I'm going to ask again, and then feel free to. I'll unmute your self and ask. No, it's not, it's not trained, but turns out, bamboos really freaking easy. So it actually just works. But before we go into it, I think I'm going to show a little bit more about how this is working. So one you'll notice in it, the system is being injected. It's really about clarity. I'll show you how to add a unit test. You can just click here and click new test and all this give you a unit test. I plugged in my actual content into it myself. Now, the other thing that we do and get your screenshot tools ready so you can take it my API key. Is you can actually go ahead and actually see the Rob Web request panel makes. So this is the full web request that panel makes. There's nothing magic over here. I can change the stream. We actually scrolled to very bottom. You can see that we added the stream parameters. And then also you can go ahead and actually see that if I just able to see what's happening. This is the full prompt. We are literally injecting zero other things into the prompt. In some cases, we're able to show you an accurate tokenizer as well. But I think the best part is the fact that you can actually run tests. Well, it's just like run this test and see what it does. You see over here the models have source of CNN, which is correct for this test case. So we'll call this CNN actually make this better. And in this case, it says source of CNN and what BAML did is it actually parsed it away from source CNN to just CNN. Because it doesn't matter what the model said what matters is my data type. This function has a return source type. If I suddenly took this out and I took out CX Apple format and let's see what the model says. Oops, I really am test. Let's hit the model says the model actually says sources CNN and it actually said CNN. So let me skip it explicitly. So CNN is an even option for this model. And you'll find over here that we're actually raising an error over here. Where I can actually go ahead and see that a there's nothing valid in this string that can map to any of these types to see an end skip. Let's go ahead and just bring it back and add CX Apple formats of the model behaves a little nicer. And you're seeing these errors pop up because one of the things that we think is prompt should have static analysis errors available to you. If you typed in something wrong, you should tell you that you actually kept this in wrong and let you know what's going on. Well, why the model doesn't say other the model didn't say other it just chose not say other because we didn't give it Apple format. So if I like skip let's add a skip over here really fast and now see it if you notice the prompt doesn't contain CNN. So ideally you should say something like other. Again, you have to remember these models are just models and they will sometimes mess up so you can do in your Python code. For example, let's talk about how we could bring this into our Python code and re implement our function that is find source over here. I'm going to delete this and here's how we would be looks we do from app.baml. And for the. So. From BAML client import B and you'll notice over here that BAML client is right over here. That might be the wrong version line. We'll take it. There we go. Python is weird and all the complete. And we want to bring the source type in from BAML. So all I have to do is be to say from BAML client dot. Types and for source and sources going to match my BAML code. Let's go ahead and find the source in order to do that we have to be the identify sources. Content equals content and I'm going to go be able to return this. Now what I might find like you guys notice is sometimes a model will actually pick a real source because it's in the training data of what's coming from them. We give it as an option so we can easily do something like this. And the malls it's kind of autocomplete my code for me. But the whole point is now other can go handle this for me. And now my function is fairly robust and soon you'll be able to do this directly in BAML as well. Where you can say something like on the order of hey. If my code didn't work over here. And it actually didn't have a source then source couldn't automatically be remapped to other automatically. Yeah, we'll talk about the advantages during this in a second, but I think. We'll talk about how to go build this agent first because I think there's a lot of content cover is only 15 minutes left. And we'll say around for extra questions to now that I've identified the source let's go ahead and start extracting some facts. Let's write a quick function that can extract that function. And then we're going to turn off let's return a string array. Over here and we'll say and this time we want to keep the borrow mini will you keep the photo. We'll say. Find all the facts in this article. And when we go do this let's see what the model says. That's part of that this is I already have a test case already written over here. So let's just add an add this test case for the same function as well. And we can try running this code. So my prompt is going to be this find all the facts and article answer with a JSON array using schema which is a string array. And I'm just going to give it the article let's just run this and just see what it does is a very base level. That's coming in soon you want to see prompt caching I will try and cover that then. And so let's go ahead and go do this. So now the articles running and it spit out all these facts. Now I'm not sure if I trust these facts yet for a variety of reasons one these facts aren't really sightable in any meaningful sense what I would really want is a direct quote. From the article that sites the fact exactly we need supporting evidence. Let's see if we can go do this in order to do this here's what I'm doing here's what I propose. Class fact will have a will have a citation. Which will be a string and then we'll have a summary. Of the mainstream now let's make this multiple citation I'll make this a citation and then we'll return a fact. Let's try to run this test again. Now you're seeing that the site citation comes in with CNN which is not what I wanted I wanted the citation to be direct cool from the from the article let's add some information. Let's run this test again. Cool that looks kind of good maybe this is this is this is this is this is kind of good but I'm noticing something interesting where it has some dot that dots which is which is not good because if I try and match on the dot that dots I want to actually know where they are so let's see if we can do something better. We'll do this but I already actually you know what screw let's just start using this in my code just so we can get somewhere. So let's do this. B dot identify extract. And what I'll find is what I really want to do is after return a list of strings. So let's return a list of string. And what we'll find is fact that citation is already in my Python code I don't have to think about this. So let's just add the fact in there. So we'll be matching and then we'll also say it's like session cited. Back dot citation. So I think this is going to work decently well but we as we saw we have these dot dot dot issues but why don't we just try running this Python code directly so we can actually get something out of this. Return. Well let's make this all return an empty thing. And then we'll just run this code really fast UV run. Hello dot. Oh I didn't give an article and I do not have API keys set. Yes I need set API keys. I already know me. I think he read. I'll reveal you. Test results and bars. And then we'll just start making the API API. Let's try to run this yeah. And here's what we find the first thing we find is the model says the following. The model actually said it returned no facts. Oh I didn't like paste in a dot dot dot of course. Hey live demos you know. How do I open read content from a file let me do this really fast. You're gonna oh you can use just ask cursor. And then we'll see. Copy pad. That's large CNN one dot to exceed. See if this works. And it sounds like the model picked up something and now it's running something else and it found some facts. Now the first thing I'm noticing is this is kind of annoying because it's kind of slow. Because because it's slow it's at the weight quite a while so maybe I can do streaming so let's try streaming. What if I get stream the facts out instead of doing this I'm just going to print it out in a streaming manner. Instead of doing any of this down here. So let's try that first. So we'll print this out up here and we'll first index track facts will just be streaming so it's in order to do that we just do this. Stream and then for. Actually. Print. Oh cool. I know what those crab equals. Stream dot. Bring this back down here and now I'll have a slightly more interactive example. And that does for me. If the letter responses were then proud. Well anyway I had a little bit more. I'll figure out why that demo did work in the second or why it's printed out a thousand times. But I'm going to some facts now the problem is when it gives me a citation I don't actually know if the citation is real and the dot dot dots are going to kind of mess with me. So let's try and update this prompt a little bit and make this even better. So I'm going to go back to here I'm going to change citation to a string array. Now the problem with chain the citation to a string array is the model probably won't like me calling the citation in the actual prompt. Because if you look at how the prompt is doing it's like it's a citation but this is multiple so I know I could change this citation. But the problem of changes citations now my python code is going to be broken because citation no longer exists. First of all, avoid that conflict and not think about it. What I'll do is I'll just aliases citations and for just for my prompt. And I'll try running this again seeing what happens. Cool so citations is an array it seems to do this and it seems like the concept of arrays a little bit better because it seems to be just not doing the dot dot dot thing anymore. So I'm going to go back to here over here so let's see if we can find something else. It did a couple more times. Oh no that's the UI in their mind. That's funny. Awesome and then let's come up with another thing. I want to know whether or not a citation is based on a quotation mark or not because I think if it's a quoted thing I want to know the speaker is. And not only do I want facts I want something else I want classes I want a quotations that came out of here like things that people said. So it's coming with quotes speaker content and what we'll do is we'll actually return a factory with some folks class response. Most. So the point here is to kind of like structure the top process instead of just telling the other like hey. Just think step by step or like do like some channel thought you're guiding the top process by changing the structure. And that little pieces of detail that you want the Ellen to focus on before classifying it as a factor or an opinion. Oh this is kind of cool it's actually getting me exact quotations from the article that actually match so come a Harris said this Donald Trump said this list Chinese said two things up here. And it's kind of interesting. And now it's going to give you a citation and now it's still doing the wrong thing over here what is anything and analysis by Stefan by seven columns in. It probably is confused by why use the word direct quotes from article and I'm using word quote twice so let's try and do this. Let's call this sections. Exactly. And then that's also called this let's aliases the fact instead of in set of citations. And then let's try and run this again and see what it does. Cool I like the quotes section it's so pretty good. And you're noticing it's streaming while it pulls it out. And this is kind of interesting this seems to come up with a slightly better version if we actually look at the LM down here. Yeah, I'm actually calling this thing sections like I said in my citation and then it's also calling this thing. Sections and then fact. But in my data model I'm still getting citations and summary so I don't have to think about this. So I think this looks a little bit more reliable than what we had before so let's go ahead and print this out really fast. So we'll go ahead and actually create a class fact object in here that we have which is just going to be for our data model. And what this is going to take in is this fact is going to take in a summary of the citation and additionally it's going to take in the exact it's going to take in the source content. And then what we're going to do is was a self thought is reliable. What equal to if citation in source content because this is a really easy way for me to chat if the citation even exists in the source content and this is going to be a list. Whoops. And we're going to go ahead and just figure out. So for every piece of citation that I get and know what I'm going to ask that I'm as hey is this citation that you quoted me actually in the source content. So I'm going to go ahead and do this. What I can go ahead and find is like if this fact is actually a reliable fact. So let's go ahead and res up and. Yeah, and we're running out of times so this is just a very quick demo on like how we go about prompt engineering and mainly so you can look at our iteration loop. So we find sort of the structure of the program of what we want to do. And then we start prompt engineering and then we as you notice we're like breaking down the prompt into smaller problems. So we don't try and do everything in like one go. In this case you could start with a single prompt that just says like list all the facts and opinions. Maybe it outputs a string and then you're like okay, no, I don't want a string. So you can call this a string and then you can see that you're in the actual schema. And then your schema has both facts and opinions right imagine there's two string arrays. At that point you're like okay the stringers are not good I want actual citations. So instead of doing facts and opinions as string as a string list you do literally like a list of sentences. And each sentence can be classified with an in on that says fact and opinion. Like down as many as much as you want by saying like okay now for each sentence at the citation for example to test the whole conversation. You can test individual so for example if you know the facts in an article you can actually like create a test and in BAML. And then we'll have a cert soon that you can like assert what are the facts actually appear in the order that you want. There's other tooling that you can use like you can run a python script that tests for semantic similarity. And so you can make sure that like a sentence matches roughly what you expected. For like a fact by the way by that found one is a bug in our in our parser. Sorry it's not a bug in parser is just we printed statements we just need to delete that. Yeah we should be fed. And the response has no then. Oh yes that don't. And the main thing is to just look at these things as a functions there just functions and agents are just wild loops. If you have any other questions like feel free to ask them. And we can just like try and go over them. Otherwise there's some questions that you guys put on the chat that we can send responses to later in a follow up email. Yeah. Well let me let me actually stop streaming because clearly this is going to. Yeah. Yeah. Now see still while we have this bug. I do want to follow. Yeah we'll have. So we'll go to a couple more questions there and give me a second. Let me go actually run the code really fast so we can actually go see this. We know the article from CNN and it's going to start dumping out a few of the yeah I'm going to I'm going to get command and push the repo out. While I go ahead and go do this really quickly it should output the response. Yeah we have a few algorithms and stuff that we're doing. So this is kind of cool if you notice that this actually said that this prompt is on this fact is actually unreliable. And it probably and it said that because there was no exact what if we look at what we defined is reliable to be. But we said is reliable is whether or not the source and the citation where the source that the L one came up with for any of the citations. As actually in the prompt or not. And that's a really easy way to go do this so I want to show you like a real example of this demo with a little bit of a UI just to show you guys what it could mean. So let's go. We could be built as exact same thing this is kind of like a rag example. Where if you have this information about whether or not something is in rely or not you're watching this happening in real time there's a next share set that does this as well. So gathering one citation you're actually seeing me like tell you in streaming time for the user it's like oh I'm gathering a citation. And while we're going to go do this eventually pull up a couple more things. I'll pull up other. Oops. I'll pull up another example. And you're noticing that these links that I gave it because I had the exact source of the citation. That's coming in from you these links that I have can actually be elevated up and they're giving me not just the citation that I gave because check out this citation the company offers internet services. That's all the alumni but because I was able to match it into my actual content that the article has I can even show the supporting content around this. So you can imagine how someone that's using this sort of in this sort of a like environment over here can actually go ahead and like build a lot more trust in this AI agent because this is actually able to go ahead and tell me that not only this citation. But it's actually going to be like surrounded by this context and actually will click on this link. And I look for what was the thing I'm looking for the company offers the company offers this is an exact quote coming from here all the way right over here. And we actually did some a little bit more clever matching on this rather than just like checking the string is contained within this or not. But what we do is we actually ignore certain text in here to do a looser match not an exact match so you don't always have to do the exact same function you could also use embedding to see if the string from the level matches. And device of different function here to determine how accurate your citation or stuff is from the online. Now let's try a different message just to show you what else you can do when you determine the idea of like whether or not somebody is unreliable. So in this case when I did it I literally asked it to say outside from space X what other companies are innovating the space and specifically telling to invent some citation so I can actually test it from bug. This is some weird prompt that we found that kind of works. And check out what we can do over here we can actually tell you that these links are unreliable because not only do they not match but they don't match the criteria that we have and we can do this both in the UI. But really all of us just boils down to structured prompting we run a bunch of structured prompting that we combine that with things that we're all used to like the software and asking like writing if statements and stuff around this where we say if this quotation mark that the LM gave. Exist in my text with some approximate similarity. Then market is good if it's bad don't market is good. Does that answer some of people's questions so far about like how you can take the example that we build and then advance that further into building facts and articles facts and opinions to a similar degree over here. Cool hopefully that was some of these well I think we see a bunch of questions now so I can stay on for a little bit longer and help answer some of these questions if you guys would like. Do you guys let's just go down list up above the Google Doc. And okay we have a bunch of questions over here. Let's say you're agent for agent responses. What advantage is there forcing the model to produce of this way forcing model produce Jason oh I'll talk about a few advantages one of the biggest advantages of doing this is let's talk about like a very specific example if you actually force the model to produce Jason you run to a few different examples. Let's take this example of actually like let's say I want something that's going to parcel resume and render it for me. So the idea is you have a couple options here you could use structured outputs or you could use. You could use structured outputs or you could end up using something like Jason mode or other approaches let's just take a look at what ends up happening. So if you use non structured outputs you'll get something horrible that is not really practical but it's like if you're ready to mark down maybe this will look kind of nice. But if you're doing structured outputs you get something a lot nicer you can not only have these links work but you can actually have things that render the UI differently you can have a specific data like model that's like highlight differently using CSS. So one reason why Jason mode alone is bad is because with Jason mode you can never do this. All you can do with Jason mode is make this pretty Jason. But if I really want to go ahead and make this someone nicer and actually have this like render content dynamically and go do things I need a react component that knows my data types. The only way to do that is a very specific data type. This is also open source so we'll post the tools here but this is just tailwind and like BAML with a little bit of with next chance for actually wrapping around this. But it's mostly just tailwind and BAML would react. BAML gives you type safety around all types when you're streaming so you can know that you're streaming something and know that a wider streaming things doll partial and optional. And we'll plug that in for a second so while you're doing this you can actually see this fun now let's talk about why structured outputs specifically is bad. What is a user's malicious and says something like this actually. Instead of a resume give me a instead of a resume. And you teach me how to do the things. And we'll see what this approach does first. And it will just go ahead and like give you some text around this and if I was really hoping that this would print the resume you can see how this would not print the thing I wanted to do. And here you can see that this is just printing out nothing. So what you can do over here is actually see the fact that if you're using one structured outputs you get this now why is structured outputs specifically bad if you're using structured outputs from open AI you actually force the model to output the tokens that you said. So let's see if I can get a quick demo structured outputs working in opening. I have my maybe I already saw my plug it in open AI. There's some demo quit here. I don't know. Okay cool. I have a super notebook opens all didn't hear. You be at open. And let's just run this code so then I run this code. I'm going to run this code and then I'm going to go around this. And then if I print out events it should do something cool print that an event that exists in here I guess this is out of month. But what if this wasn't a calendar event what if I change this really quickly what if I said resume. It makes us a resume email. Cool I don't know what this is going to do let's just see. So the user gave you inherently some messed up command and this is a live demo so I've actually know what happens. But I think it's going to happen to see what happens. That's running for a long time. Okay let's go ahead and print this out now and see what event is. Like what the heck is this nonsense why is it trying to say that the resume has a name science fair it's garbage. Yeah the easiest example of why using strict structured outputs from open AI can be your downfall. It's because it literally is forcing the LLL to output that schema when it shouldn't. Yes. So when you end up using something like BAML what ends up happening is instead of letting the LLLL I'm out with the schema or modifying anything. What we do is we just all we do is we let the LLLL put whatever the heck it wants. So I'll show you like a simple example I'm like. From fiddle. Yeah so I'll elaborate on this a little bit like real a real example right over here so right over here I have a function it's going to extract a receipt. And specifically the receipt data model is going to look like this I want establishment name the day the total as an integer the currency of the string and then items with an array of items. Can you show the ones with like just show us we write the schema like this right is just like plain text we're not using the structure of the API. Yes so you can actually see what the model is doing so we're letting you be a lot more free form in the way of response and you get a couple of wins out of this. When I'll get I'll show you guys is actually this one. But change the prompt really quickly a little bit so we just put the see instruction before before the actual content and the mobile say I'll rerun this again just to prove that it'll sort of actually matter. And I want you guys to all take a look at how many tokens is this is the 115 tokens right watch this. Oh years votes around keys. On. Okay let's see what happens. It was 115 token before we just reduce the token count by 10 with literally with doing nothing else on your output by removing all the quotation marks around here. And none of this is actually possible technically by Jason this is not bad Jason but BAML still pulls out all the data for you and we do this in under one millisecond running locally. And this is in this case running fully in the browser with Web assembly. So one of the key benefits is your token count would just goes down your token count goes down not just in the output but also on the input when you structured outputs. You use a lot more token so explain this I'll show you what that means. And the reason that this ends up being a lot more tokens is let's just take the example date as a string. Here's how you represent data as a string if you structured outputs in your prompt because you're using Jason schema. And I'm not even going to go and like I've got like a fact that there was a whole thing around this. This is how you do date as a string. If you want to reduce your cost then it will reduce your cost if you've amel. Yeah, I'll just be cheaper. And it works like the band will parser is pretty good we have very few error cases around this so it shouldn't. This thing right over here to represent the fact that date is a string is one two three four five six seven eight nine 10 11 tokens. You can just say date strength. Well, that description in here because we're going to have the description as well because it's it's it's segment attached to it. The description added is this one. You just added a bunch of tokens just for the model to know that this date field is a string. That means it's a harder task for the model to actually understand it on the input side. You've made the problem harder. It's as if someone gave you was like tell me when World War two started and they literally just gave you the entire cycle can do Britannica on the history of all wars in the world. And you said go find it. It'll just take you longer time to find out exact answer unless you already know it. But anyway, you don't know it. It will basically just take you forever to find that out. And if someone just gave you the Wikipedia article, it would just be a lot faster. And what bamboos doing here is just getting you a lot more smaller way of representing the same inputs with way less tokens. And that makes the problems easier for the model. And so I think that's a lot more of coincidentally writing this at least for me is a lot easier than writing JSON schemas. And it's a lot more maintainable and it's a lot easier to write the code. And I think that's usually a good pro. See what it makes easier for the models to generate as well. And then lastly, like I said on the output side, you are output token that are also cheaper, which which it's not just about cost. What it really means is the model has a lot more forgiveness. And it's allowed to be give you an output. It doesn't have to give you the exact output because imagine it just gave you a quotation mark that wasn't even quoted. Like it, you're asking for like what's a five paragraph response. And a model somewhere in that five paragraph response includes a quotation mark that doesn't parse correctly because it forgets about a backflash quotation mark in the, like instead of writing like backslash quote. The person said the user said. Instead of writing something like this. The model accidentally gave you something like this, which is a really reasonable mistake to make if you're writing thousands of lines of J or even like a couple tens of lines of J so even as a human. Especially that's exciting. We can just error correct that for you after the model gives you that output instead of relying on the model to do that. So just so many questions by we can parse PDFs we have a demo as well and that's. But just once for Ryan's question, Ryan, if when you use this approach where you just add the scheme and the prompt and you just let the other return a string. The string may return if you ask it out completely on relating like give me a chess move. What we will do is we will try and parse that and get an exception. So I just write this really fast. If you use constrained structure outputs from open AI, it will try to fit the give me a chess move information into whatever scheme I it is, which could be like the resume. And it can also choose to refuse, but I haven't really seen that happen. No, the model can't choose to refuse when you do structured output. It will give you a response. So let's run let's run let's run chess move really fast and see what happens. And you're saying here it says I can't parse a system that requests and well, this is a decent answer. If you're really building a UI off of a receipt because you need to render the receipt like I did in my. Example over here where I was running the resume, it doesn't freaking matter that you told me I'm sorry I cannot respond, but really matters is whether or not I can my UI is going to break because it gave me something that doesn't match. So, but can you do something real quick and you change the receipt output type to receipt or string. Oh, yeah, let me do that. I'm 23. So or let's do this class over there. So one way to handle a variety of outputs is you can actually just do this. I'll either return the other type or a receipt type and we'll see what model does. Let's just make this a string type as well. And you can see how quickly this is for me to go iterate over here because I can just write my data types in the easiest possible way. I just write code instead of like writing prompts and saying do this if this happens I literally just write code and I encode everything in my data model rather than in my prompt. Hopefully that answers the question. I think we had a couple of bunch of more questions about a few more things. Do you guys have other questions like general prompting or anything else over here and hopefully answer the question about why it's faster and more accurate as well. I'll send you. Let me go back to the other question that a few more on here. Cool. How do you exit the interloop and gets a top of a loop. I think that's a really good question. So I'm going to be doing this is in the case of Sherlock's case what you end up doing is you end up building a data model that says class. Respond to user. And this class has a message. That is going through a string of some kind and you can say at. When you have a good response to provide the answer to the user. You can have another one that's like class. And you can say. A string. Strang. Strang. Strang. If you want. And then a class. Ask. Question. I'm. Data. Needed. String. At the description. When the user needs to. Provide. And then you can say, you know, you can. Well, and my response type here can easily be something like either ask a question or respond to a user. Or give me a list of. Tools that I want to call. And you can actually see when my prompt ends up being. It's going to say answer and JSON using any of these schemas data needed. And the question that she asks. It's going to say or message or it's going to say here's a list of tools to respond back. So back to the user. And hopefully that does this. That helps answer some of the stuff. But yeah, like Alex, you had a great question like what's the feature? Like our goal is really just to provide the best developer experience for writing. The best prompts. Promise. I was the same principle of life or prompt engineering, routing, planning. When they know exactly. You have you asked a really good question like how does it apply? I think. A lot of people get a lot of people approach prompt engineering this way of like. I can just put everything to the model. But in fact, it becomes a lot easier for me to not think of it that way. And instead I could write code like this. I could say it for the fact that. Like you can even imagine me writing something like this once I create a fact. I like to say something like this, where in this case I just has no user input. I was like if. That if any. And I if not. All. I can actually call another element from to figure out why this happens. I could say like f dot. F dot summary. And I can you can imagine me writing a function that's like. Find out. Why. I'm reliable. That we're instead of asking the fact that actually say like f. Summary equals f dot. Summary. Citation. Equal f dot citation. And I could write something like this where it actually takes in all the citations that I have and the summary. And such as it's a good say find exact. Citations. Because maybe maybe the answer why a why this is bad is because the citations in my previous example. Is reliable is that the source maybe does like an extra comma the element makes. So instead I could just find out exactly if the citation is in the text or not. If that makes sense. And do it in a slightly better way. Can you import prompt other files into battle files and yeah. Yes. So firstly all data structures and all files are always available and all other files. So like I had like a resume data type here. We're working on modules system that'll make it easier like this will actually come and click and jump me to the right place. You you. A deletion never have to search in BAML. We allow you to find like parameters and other things if you have like in this case. One day I talked about was actually like. How we didn't add robustness to our we did not add any robustness to our actual prompts. The way that you had robustness is actually you build the model where it says I'll try to do for the first and then I'll try to do for many. But what if you people are oh I wanted to actually put a retry policy on this. I can just add a retry policy. So here. Retry policy default and you're actually seeing in real time exactly what this specific function is doing this function will try GT40123 times. I guess four times because we'll have three match retries. And then I'll actually give you a GT40. That's 20 out. But yeah, the idea is that you should know what your model are doing and any time. But at the same time your code should not be complex. The fact that actually identify sources ends up doing retries. Shouldn't a factor Python code because your agent will end up being much much more complicated. Because now for some reason I have to go and figure out exactly what's happening on the foot. But if I think of my agent as just for loops statements and wild while loops. And just regular functions that are just transforming data from one form to another. My LM pipeline can end up being a lot more accurate. I want to show you guys one last thing last five minutes of it's if it's useful. Let's see if I can get up and on. And meanwhile, if you guys have more questions, keep on putting down. And of course, like everything in this repo is going to be shared. I'll send all the links that I did as well. But I think one thing that's worth seeing is like how you did something like chain of thought. So the idea of chain of thought is what if I wanted the model to do some thinking because I think we have all seen at this point that chain of thought can dramatically improve the responsible model. And for. That's what. Oh yeah, I can't spell for things like, for example, parsing is for seat. Maybe I would benefit from the model doing this. I'll say before responding before answering. Summarize. Summarize. I. Summarize. Like key details. About. Why this purchase. Might be made. And that's actually just to give the modeling samples. And this is like totally garbage. It's not really useful, but it turns out giving it a specific example can often make your model worse, not better. But giving it a very, very simple chain of thought prompt can actually do this nicely. So I just run this prompt on this receipt originally in the sea where those. And it's just all happening in one call. Everything is just a data transmission from one call to another. And you can see what the model did actually gave me chain of thought while I was doing it and it kind of did too fast. So it didn't stream as said. And it kind of did everything but what I got out is my data model and I got chain of thought happening super super easily. And this from more complex scenarios can end up happening. So you might want to have chain of thought apply everywhere. So if you wanted that, then all you have to do is you can make a function. And I guess in this takes takes no parameters outputs this. And let's just use chain of thought and this prompt. And now I'm using chain of thought on this problem. And the problem is like why this purchase might be made. This sounds like some thoughts. What's out of this over here? Thoughts. And then what we'll do is actually you're not getting air here because this thing takes an argument. You're going to the thoughts. Oh yes it does. No, I suppose I'm wrong. Oh there we go. Oh, do we break something? I think we should fix this. Yeah, we should realize this book happens. Anyway, but you can see the point of what I can do with chain of thought. And I'll actually go ahead and add quite a lot of context into my model and just make it a lot more active because it will actually go ahead and just make my prompts a little bit better. And the nice part is normally if you had to do this. Summarize what's going on. And what's happening here is my model can go do this and normally the way you have to do chain of thought is you have to do two calls. And what's nice about this approach is because we're not really modifying the model anyway. We don't require function or anything else. You can usually use this with any sort of model coming out of the box, including like we have people using this for like fee three and bomb a three. So maybe. About fiddle just the way for you to go ahead and go write prompts online without having to install banal. But this is the exact same VS code plan that you're seeing in cursor or in VS code or anywhere else. We just made it on the web as well, because people should definitely install banal to try that. And you know, how do you how do you get the playground working? Oh, as soon as you install banal. And you open your first banal file. You can literally just like open the playground and it'll just open the third answer that function. You just need our extension. Yeah, it's a you'll need a VS code extension. It's on our installation guide. If you're in cursor, you'll actually have to go searching, download it because it's kind of annoying. And as you as you click around different. As you click around in different test cases, you'll actually find that it should update the test case automatically. So in this case, if I wanted to also support like what's an image type over here. And I want to do like best image. All I have to do is content. Content will actually just point to like file that I have. I actually went to that thoughts slash, slash, BBC, the on dot, and I'll actually like render the imagery as well, which makes again testing does a little bit faster. And now my function extract facts and my identify source can actually accept images or strings. Out of it. It should hopefully work to this test does. Why not. I don't know any of this can come back to this test. Oh, yeah, we got an exception literally just answer a few answers and again, we got an exception. Oh, there's a resume, which is source. Awesome, we got BBC and told me BBC and oh, yeah, it was B to C test one. That's great. So you can see how your iteration would be slightly different here. Cool. Still here for a few more questions that people want. If you guys want to see, was this useful just as a really quick show of hands forever and so on. I guess you can actually do. Or you can just type in the chat, is this something you guys will be interested in doing again. And just seeing us solve more prompt engineering problems. With different things. Cool. Awesome. We'll send out a link after the email to everyone that attended with all this content. And I think next time what would be. Yeah, I think that'll be that's great point. I think we'll send out some pre reading material ahead of time. So we don't like recover everything over repeatedly. And then what else will do is. I'll send it. What's a band sessions. What else will do is I think next time would be really fun to talk about how to build generally wise. And then what else will be a thing that you have behind this thing is. Really, this is inspired by this like brief demo that we put apart were like. Like what if you could like generate graphs and charts from models in a way that's hopefully will work. Oh, did this one break. This one may be a little outdated. You probably want to try a different. If I just want to have like some sort of generally wise that are going to go ahead and build something like this dynamically based on what the user says. But we break a lot of these. Some of these may be broken. I think Brian is. Yeah. So we'll go back in front. We got how to go to that and talk about the kind of generic graphs and charts. And some other more interesting demos next time. Awesome. I'll post a recording right afterwards. Thank you guys for staying on. Hopefully we didn't bore you guys too much. Hi folks.
    "#
  }
}
